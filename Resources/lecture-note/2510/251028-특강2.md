개발과 배포가 어렵다!
i.e. 환경 불일치

많은 환경이 어떤 모듈이나 패키지에 의존성을 가진다.

외장 톰캣? 내장 톰캣?
개발자는 이런 환경을 다 테스트해보진 않는다. 보통 로컬에서 해보게 될 텐데, 시스템 운영자나 테스터가 이걸 확인하게 되면 문제점에 직면하게 된다.
DB driver, port가 이미 점유되는 것 등 실제로는 다양한 문제로 동작하지 않아 설치 지옥이 펼쳐질 수 있다.

실제 환경과 개발 환경은 작든 크든 항상 차이가 있다.
시스템 엔지니어가 로그를 봤더니 명령어가 없다고 뜬다. 이러면 개발자가 문제를 찾아야 됨. 우분투에는 명령어가 없어서 설치해야함. 16은 있는데 18부터는 새로 설치해야 된다~ 같은 문제

이것에 대한 가이드라인 존재


테스트하게 되면 더 어렵다
실제로 명령어를 작성할 수 있음. 
쉘 스크립트일 수도 있고, 퍼펫 스크립트로 배포 스크립트 작성할 수 있음.

스크립트 썼으니 문제 없는거 아님? -> 목표 OS가 mac, windows. 리눅스더라도 centOS나 redHat일 수도 있음.
목표 환경이 보안으로 인해 레거시 쓸 수도 있다...
시스템 엔지니어 입장에서는 어느정도 돌아가게 해달라고 하지만 모든 환경을 재현하기는 어렵다.

배포환경까지 가져가서 나온 게 가상화 기술
VM! 가장 먼저 나온 친구

하이퍼바이저라는 실제 물리적으로 가지고 있는 리소스들을 가상으로 만들어주는 기능이 있다. 가상 램, 디스크, cpu

Guest OS를 설치해주면 하이퍼바이저가 만들어놓은 가상 자원을 보게 된다.

하이퍼바이저
타입1, 타입2 존재

차이점?
베어메탈 아키텍처
아두이노, 아트메가 같은 친구들이 OS 안올리고 하는 애들

타입2는 OS 위에 OS 올리는 거


가상 머신
VMware, VirtualBox, Parallel
이 친구들을 쓰게 되면 끝내주게 느려진다.
필연적으로 성능 하락이 발생하게 됨.

그래서 나온 것이 컨테이너 개념
도커의 제반 기술

실제로는 아니지만 서로 다른 컴퓨터처럼 보이게 만든다.

네임스페이스랑 c그룹이란 기술로 컨테이너를 구현한다.

Unshare라는 기술이 있다. 네임스페이스로 공간 격리
네임스페이스를 격리하는 기술을 사용하게 되는데 다른 네임스페이스의 파일, 네트워크 등을 접근하지 못하게 막아버린다.

superuser do라는 줄임말임. 
`sudo unshare [options] bash`
확인해보면 마치 원격 접속하는 것처럼 동작한다.
프로세스의 실행 순서 예시로 격리공간 내부에서는 1번이지만 외부에서 보이는 격리공간에서는 PID가 1917(예시)로 보일 수 있다.
격리공간 내에서는 처음이지만 바깥에선 아님.

cgroup으로 리소스 격리
`cd /ksys/fs/cgoup/` 같은 거 들어가보면 cpu, io, memory 등의 제한 값을 설정해줄 수 있다.
디렉토리로 그룹을 만들어 그룹별로도 관리할 수 있다.
	디렉터리 안에 프로세스 등록 해 그룹에 포함 가능
cgroup.procs 안에 PID 등록하여 해당 그룹 제한 설정을 받게할 수 있다.

요약
namespace로 공간 격리
cgroup으로 리소스 격리
이것이 기본 개념


가상머신 vs 컨테이너
가상머신은 환경 재현이 목적, 컨테이너는 격리에 초점.
그래서 가상머신은 OS위에 OS
중간에 하이퍼바이저가 있어서 굉장히 느리기도 하다.

컨테이너는 격리 공간을 만들 뿐이지 실제로 가상화를 하지 않기 때문에 도커 엔진을 통해 명령만 전달하는 개념이다.

격리 수준 : 리눅스 커널 공유
호스트의 OS 커널을 공유

아무튼! VM의 불편함으로 등장한 것이 도커

개발자와 운영자 간의 환경차이 문제 해결
- 내 컴퓨터에서는 되는데요? 해결 가능

linux의 namespace와 cgorup으로 구현
- linux가 main os일 경우 docker 컨테이너 os가 linux라면 둘은 공간만 분리하게 됨.
즉, 성능 하락이 없다.

하지만 윈도우나 맥에서는 리눅스와 아예 다른 운영체제기 때문에 가상환경 위에서 동작한다.
윈도우 - WSL2 또는 Hyper-V 기반 리눅스 VM 안에서 도커 실행
맥 - 경량 linux VM 안에서 도커 엔진 실행

윈도우에서 돌리면 성능의 이점은 없지만, 환경 통일의 이점은 있다.

---

도커 실행하기?

국룰 `docker version`
정상적으로 설치되어있는지 확인 가능

도커에 대한 기본적인 개념은 여기서 마무리

---

# 쿠버네티스 배포와 운영

> 절대 다 알 수가 없다.
> 쿠버네티스 범위가 너무 방대하고 넓다.

근데 이게 왜 쓰는지 알아야 함.
도커는 직관적으로 받아들이지만 k8s는 와닿지 않는 경향이 있는 것 같다.


## 배포, 운영 시 문제점

### 배포환경의 변화

과거에는 배포해야 할 컴퓨터가 몇 대 없었으나 현재는 기하 급수적으로 늘어남.
> 100대도 우스운 수준이라고 함...

스크립트를 짜서 배포하게 됨.
shell, puppet, ansible.
ansible이나 puppet, shell 잘만들어두면 딸깍으로 배포 가능.
죽었는지 살았는지는 모름.

### 배포 실패

실제 환경과 개발 환경 차이로 배포 시에 실패할 수 있다.
사용자는 정말 이상한 사람이 많아서 온갖 값 다 때려박아서 해킹하려는 사람도 있다.

배포 도중 실패할 경우 재빠르게 롤백할 수 있어야 한다.
그렇지 않으면 사용자 이탈로 이어진다.

### 서버 죽음

굉장히 많이 죽는다.
오랫동안 눈치 못채고 있는 경우도 있다.
잘돌아가는 서버도 트래픽 증가, 메모리 부족 등의 다양한 이유로 죽을 수 있다.

- 죽을 경우 일일이 원인 확인하고 복구시도, 혹은 롤백 시도
- 장애 발생 시 바로 파악 못하고 대처 늦을 수 있다.
- 이 과정에서 많은 시간이 흐름.

재빠른 복구로 이어지지 않는다면 사용자 이탈로 이어진다.

## 오케스트레이션

여러 개의 서버, 어플리케이션, 컨테이너를 자동으로 배치하고 운영하도록 관리하는 것을 일컫는다.

컨테이너(도커 이미지) 자동 배치, 실행, 복구, 확장
사람이 직접 하지 않고 쿠버네티스가 자동으로 한다.
서버 죽을 시 자동 재시작
어플리케이션 업데이트를 무중단 배포
트래픽에 따른 자동 스케일링
복잡한 배포 과정 자동화


서버 점검할 때 배포를 많이 하는데, 그런 경우가 잘 없다.
그 이유는 요즘은 무중단 배포를 하기 때문이다.


## 구현체

이걸 보면 스펙만 있고 내부적인 구현체는 다 따로있다.

On premise를 위한 kubeadm

클라우드 환경을 위한 aws eks, azure aks, google gke

실제 환경에서 쓰이는 쿠버네티스는 테스트 하기에 설정이 많고 무겁다.

따라서 minikube, kind를 주로 테스트 시 사용한다.
kind는 docker in docker...
minikube를 예전에 많이 사용했으나 요즘은 kind도 많이 사용함.

환경 통일을 위해 minikube 사용함.
실행 시에는 `minikube start` 해주면 된다.

키면 docker ps 해보면 minikube 떠있는 거 확인 가능함.

`kubectl get pod -A`
띄워둔 pod를 다 확인 가능함.


## 실행 단위 - pod

가장 최소의 쿠버네티스 실행단위 (기본 실행 단위)
하나 이상의 컨테이너를 담는 논리적 공간
- 실제로 메인이 되는 컨테이너는 한 개만 있음.
- 일반적 인식 1 pod = 1 app = 1 main container
실제로 직접 생성x 간접 생성

pod는 yml을 통해 생성
manifest?

앱 실행 로그 확인

yml에서 `-` 이거 뭐임?
배열로 데이터 받아줄 때 써준다.

kubectl log -f [pod 명] -c [컨테이너 명] (pod는 안 붙여줘도 실행은 됨)

kubectl exec -it [pod명] -- [명령어]
컨테이너 여러개면 
kubectl exec -it [pod명] -c [컨테이너명] -- [명령어]

`docker-compose` 명령어는 요즘은 `docker compose`로 사용하는 것이 권장됨.

kubectl describe pod [pod 명]
이렇게 되면 현재 pod의 상태를 볼 수 있다.

수정하기
kubectl edit pod [pod 명]

기존 manifest.yml의 설정을 수정해줄 수 있다.
기본적으로 vim으로 열린다. 윈도우에서는 메모장 켜짐. 이것도 설정할 수 있다!

삭제하기
kubectl delete pod [pod 명]
해주면 지워진다!

kubectl에선 stop은 없고 삭제만 존재함.

로컬에서 체크하기
kubectl port-forward pod/[pod 명] `로컬포트`:`컨테이너포트`
이처럼 포트 포워딩을 해줘야 함.

## replica set

일일이 2000개의 pod를 생성해줄 수 없다.
여러 개를 복제해주고 싶을 때 사용하는 기능임.

pod를 생성하는 가장 일반적 방법 (stateful set, daemon set도 있지만 설명 x)
pod를 일정 수로 유지하려 한다.
늘어나면 죽이고, 죽으면 살린다.

이런 replica set이나 pod같은 것을 resource(Kind)라고 한다.

manifeset 속성 설명

spec.replicas
rs에서 생성하고 관리할 replica 수
spec.selector

spec.template
하위 속성들은 pod 속성과 동일. 그래서 관리 대상이 된다.
spec.template.metadata.labels.<레이블명>
pod에 레이블을 붙임.
기존 pod 만들때는 사용안함.
- 이러면 어떤 레플리카도 관여 불가
레이블 지정해서 rs가 pod 만들 때 해당 레이블 붙임.
- pod들이 이제 rs의 관리대상이 된다.

그런데 만약 기존의 pod에다가 label을 붙여주면 어떻게 될까?
rs는 정해진 수의 레플리카만 관리하려고 한다.
그래서 라벨로 관리할 때 기존 rs와 단일 pod 중 아무거나 제거하게 됨.

`kubectl delete rs my-pod-rs` 해주게 되면 다 죽음.


## deployment

rs에서 한 단계 더 나아간 것임.
pod.yml 파일을 만들 필요가 없다고 했지만, 사실 deployment 가 replica-set도 만들어주기 때문에 rs도 만들어줄 필요가 없다.

- replica set을 만든다.
replica set의 모든 기능 사용
- 무중단 배포 (여러 전략 존재)
- 세이브 포인트를 통한 쉬운 롤백

manifest도 보면 kind가 Deployment인 것을 제외하면 거의 똑같다.

`kubectl apply -f [yml]`
차이점이 있다면 deployment가 rs를 생성한다. pod는 그 rs에 의해 생성됨. 하청의 하청!

![[Pasted image 20251028172118.png]]

template의 일부가 바뀌어도 이를 감지해서 재배포가 일어남.
기존 rs가 삭제되지는 않음.

배포 전략
![[Pasted image 20251028172527.png]]

이벤트롤 통한 동작 확인
![[Pasted image 20251028172750.png]]

업데이트 확인해보면 8개에서 25%니까 2개를 새로 생성하는 것을 볼 수 있음.
그래서 잠시동안 10개.


기본적 속성
![[Pasted image 20251028173053.png]]

롤백 시 DB pod는 어떻게 되나요?
-> DB 데이터는 상관 없다.
-> k8s도 볼륨이 똑같이 존재해서 DB 데이터는 볼륨으로 연결되어있기 때문에 롤백 하던 말던 상관이 없다.

### 재시작 하고 싶어용

새버전이 아니라 그냥 재시작을 해주고 싶다?
`kubectl rollout restart`를 통해 재시작 가능
완전 같지는 않지만, 내용 자체는 같은 rs를 사용해서 재배포함.
서버 하나가 안 좋을 때 일괄 재시작 해주고 싶어 할 때 이런거 많이 씀.

`rollout undo deploy my-pod-deploy`
이거 하면 롤백
중요한 건 롤백 이전 버전 rs가 안사라짐
- 문제 될 수 있기 때문에 delete로 삭제하는 것을 추천

### 특정 버전으로 가고 싶어용
`kubectl rollout history deploy my-pod-deploy`
배포 기록 확인
revision은 rs의 버전을 의미한다.

`kubectl describe rs` 해보면 annotations에서 deployments....revison을 확인 가능

--to-revision=`version`
이런 옵션을 추가해주면 특정 버전으로 갈 수 있다. 현재 버전으로는 못감.
돌아가고 나면 기존 번호는 사라지고 새로운 번호가 부여됨.
만약 3번으로 이동 시 3번이 사라지고 5번이 새로 부여된다.


## 서비스

매우 추상적인 개념
L4 로드밸런서라고 생각하면 된다.

좀 더 고차원 적인 개념이긴 하다!

쿠버네티스의 필수 개념
컨테이너, pod, node, cluster

cluster는 node n개
node는 pod n개
pod는 container n개

노드 = 서버 = 컴퓨터 1대

minikube는 기본적으로 단일노드, 테스트용으로만 사용
> 설정으로 다중 노드로 바꿔줄 수는 있다.
> 다중 노드여야 확인할 수 있는 것들이 많지만 방대해지고 네트워크 지식까지 동원해서 설명해줘야 함.


Pod라는 건 결국 각각의 ip를 가지게 된다.
단일 진입점이 필요해지게 된다.

pod ip를 확인해서 쿠버네티스에서만 사용가능한 ip이긴 하지만, 요청을 보낼 순 있다!

하지만 불가능하다. 그 이유?
pod의 이름, 수, ip가 계속 바뀔 수 있다.

이 상황에서 안정적으로 어플리케이션을 제공 위해서는 하나의 진입점이 필요함.


서비스는 총 4종류 존재

clusterIp NodePort, LoadBalancer, externalname

### Cluster IP
내부-내부 연결

원리?
core dns
- 서비스 이름으로 검색해서 Cluster IP 주소를 가져오는 기능
- 만약 내부에 Cluster IP가 캐시되어있다면 core dns에 질의하지 않음.
iptables
- 리눅스 시스템에 있는 요소이며 pod가 아니라 node에 존재
- Cluster IP와 pod IP의 매핑 정보가 들어있음.
	- 1:N
kube proxy
- 리소스 변화를 감지해서 iptables를 변환시킴

![[Pasted image 20251028174735.png]]


spec.type
서비스 타입 의미
spec.selector.레이블 명
- 가장 중요
- 셀렉터와 같은 레이블의 pod가 대상이 됨
- 레이블이 복수면 모두 일치해야 대상
	- and 연산
ports
- pod가 연 포트를 해당 서비스의 포트에 붙인다
- port는 pod 내부, targetport는 변환할 포트

`kubectl get svc`로 cluster ip 조회 가능
`kubectl port-forward svc/my-pod-service 80:80`
하게 되면 우리 80과 해당 컨테이너의 포트를 연결할 수 있게 된다.

라운드로빈하게 분배한다는 것은 각 요청마다 pod를 연결해준 다는 뜻...


### Node Port

외부-내부의 연결

cluster ip와 차이점은 node에 포트 포워딩을 해서 노드로 접근가능
cluster ip + 포트 포워딩

세팅은 cluster ip에서 바뀌는 것만... 바로 넘어가셨다!

cluster ip일 때는 `get svc`해보면 80/tcp
node port 세팅 시 80:30080/tcp로 바뀐 것을 확인 가능

원래는 external-ip가 있어서 외부에서 접근 가능한데, 미니쿠베는 못함.
ip:node port 조합으로 원래는 요청이 가능하다!

### Load Balancer

node port, cluster ip 모두 포함
aws alb, azure load balancer, metal lb
이것들을 등록하고 사용해야 실제로 사용이 된다.

타입만 LoadBalancer로 지정해주면 ClusterIP에서 바뀌는 건 없다.

미니쿠베는 가상으로 세팅해줌.
ip는 loop back.


---
QnA

쿠버네티스에 대한 질문이 온다면 대답을 해주는 정도는 올 수 있다.

개발자가 얼마나 알아야 할까?
내가 뭐 더 알아야 하는 거 아닌가? 같은 것 보단 높은 이해도를 가지고 있다는 것을 보여주면 좋다.
